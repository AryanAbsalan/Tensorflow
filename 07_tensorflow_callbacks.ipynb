{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>PressureLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.374540</td>\n",
       "      <td>0.950714</td>\n",
       "      <td>0.731994</td>\n",
       "      <td>0.598658</td>\n",
       "      <td>0.156019</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.155995</td>\n",
       "      <td>0.058084</td>\n",
       "      <td>0.866176</td>\n",
       "      <td>0.601115</td>\n",
       "      <td>0.708073</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020584</td>\n",
       "      <td>0.969910</td>\n",
       "      <td>0.832443</td>\n",
       "      <td>0.212339</td>\n",
       "      <td>0.181825</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.183405</td>\n",
       "      <td>0.304242</td>\n",
       "      <td>0.524756</td>\n",
       "      <td>0.431945</td>\n",
       "      <td>0.291229</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.611853</td>\n",
       "      <td>0.139494</td>\n",
       "      <td>0.292145</td>\n",
       "      <td>0.366362</td>\n",
       "      <td>0.456070</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3  feature_4  feature_5  PressureLevel\n",
       "0   0.374540   0.950714   0.731994   0.598658   0.156019             50\n",
       "1   0.155995   0.058084   0.866176   0.601115   0.708073             70\n",
       "2   0.020584   0.969910   0.832443   0.212339   0.181825             95\n",
       "3   0.183405   0.304242   0.524756   0.431945   0.291229             47\n",
       "4   0.611853   0.139494   0.292145   0.366362   0.456070             18"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a dataset with 1000 samples and 5 feature columns\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "\n",
    "# Generate random feature data (values between 0 and 1)\n",
    "features_data = np.random.rand(n_samples, n_features)\n",
    "\n",
    "# Generate random target data (PressureLevel) - using values between 0 and 100\n",
    "target_data = np.random.randint(0, 100, size=n_samples)\n",
    "\n",
    "# Convert to a DataFrame for features and a Series for the target\n",
    "columns = [f'feature_{i}' for i in range(1, n_features+1)]\n",
    "df = pd.DataFrame(features_data, columns=columns)\n",
    "df['PressureLevel'] = target_data\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (800, 5)\n",
      "X_test shape: (200, 5)\n",
      "y_train shape: (800,)\n",
      "y_test shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into features and target\n",
    "features = df.drop(['PressureLevel'], axis=1)\n",
    "target = df['PressureLevel']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aryan\\AppData\\Local\\Temp\\ipykernel_13048\\3483093910.py:5: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y_train = y_train.ravel().reshape(-1, 1)\n",
      "C:\\Users\\aryan\\AppData\\Local\\Temp\\ipykernel_13048\\3483093910.py:6: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  y_test = y_test.ravel().reshape(-1, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "y_train = y_train.ravel().reshape(-1, 1)\n",
    "y_test = y_test.ravel().reshape(-1, 1)\n",
    "y_scaler = StandardScaler().fit(y_train)\n",
    "\n",
    "# Features\n",
    "X_train = X_scaler.transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "\n",
    "# Target\n",
    "y_train = y_scaler.transform(y_train)\n",
    "y_test = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='tanh', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=[tf.keras.metrics.MeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TerminateOnNan callback allows to stop the training when the loss function has diverged (by NaN value). \n",
    "\n",
    "This callback can be useful to avoid continuing to train the model if it has diverged or to stop the computation in case of non conforming data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.2455 - mean_absolute_error: 1.2365 - val_loss: 2.3201 - val_mean_absolute_error: 1.2446\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0466 - mean_absolute_error: 1.1756 - val_loss: 2.0725 - val_mean_absolute_error: 1.1744\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8055 - mean_absolute_error: 1.1174 - val_loss: 1.8751 - val_mean_absolute_error: 1.1203\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5742 - mean_absolute_error: 1.0259 - val_loss: 1.7162 - val_mean_absolute_error: 1.0776\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5331 - mean_absolute_error: 1.0401 - val_loss: 1.5807 - val_mean_absolute_error: 1.0393\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3773 - mean_absolute_error: 0.9883 - val_loss: 1.4734 - val_mean_absolute_error: 1.0095\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2999 - mean_absolute_error: 0.9598 - val_loss: 1.3830 - val_mean_absolute_error: 0.9852\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3088 - mean_absolute_error: 0.9746 - val_loss: 1.3077 - val_mean_absolute_error: 0.9646\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1575 - mean_absolute_error: 0.9079 - val_loss: 1.2474 - val_mean_absolute_error: 0.9472\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1819 - mean_absolute_error: 0.9226 - val_loss: 1.1960 - val_mean_absolute_error: 0.9315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d8a6e90830>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "TON = callbacks.TerminateOnNaN()\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32, \n",
    "          epochs=10,\n",
    "          validation_split=0.2, \n",
    "          callbacks=[TON])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LearningRateScheduler is a callback which allows you to vary the learning rate during training.\n",
    "\n",
    "\n",
    "This callback takes as argument a function that updates the learning rate according to the epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 1/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1699 - mean_absolute_error: 0.9228 - val_loss: 1.1915 - val_mean_absolute_error: 0.9301 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 2/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1136 - mean_absolute_error: 0.9000 - val_loss: 1.1872 - val_mean_absolute_error: 0.9287 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.00010000000474974513.\n",
      "Epoch 3/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0831 - mean_absolute_error: 0.8789 - val_loss: 1.1827 - val_mean_absolute_error: 0.9272 - learning_rate: 1.0000e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 1.0000000474974514e-05.\n",
      "Epoch 4/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0797 - mean_absolute_error: 0.8731 - val_loss: 1.1823 - val_mean_absolute_error: 0.9271 - learning_rate: 1.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 1.0000000656873453e-05.\n",
      "Epoch 5/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1464 - mean_absolute_error: 0.9149 - val_loss: 1.1818 - val_mean_absolute_error: 0.9269 - learning_rate: 1.0000e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 1.0000000656873453e-05.\n",
      "Epoch 6/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0607 - mean_absolute_error: 0.8684 - val_loss: 1.1814 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-05\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 1.0000000656873453e-06.\n",
      "Epoch 7/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1480 - mean_absolute_error: 0.9154 - val_loss: 1.1813 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-06\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 1.0000001111620804e-06.\n",
      "Epoch 8/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1543 - mean_absolute_error: 0.9191 - val_loss: 1.1813 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-06\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 1.0000001111620804e-06.\n",
      "Epoch 9/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1672 - mean_absolute_error: 0.9250 - val_loss: 1.1813 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-06\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 1.0000001111620805e-07.\n",
      "Epoch 10/10\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0380 - mean_absolute_error: 0.8602 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d8a71252b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the current epoch number is divisible by 3 (epoch % 3 == 0), \n",
    "# the learning rate is reduced to 10% of its previous value (learning_rate * 0.1).\n",
    "\n",
    "def decreasinglrUpdate(epoch, learning_rate):\n",
    "    if epoch % 3 == 0:\n",
    "        return learning_rate * 0.1\n",
    "    else:\n",
    "        return learning_rate\n",
    "\n",
    "\n",
    "lrScheduler = callbacks.LearningRateScheduler(schedule=decreasinglrUpdate, verbose=1)\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32, \n",
    "          epochs=10,\n",
    "          validation_split=0.2, \n",
    "          callbacks=[lrScheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EarlyStopping is a widely used callback. It allows you to control the evolution of the metrics by stopping the training when they no longer improve.\n",
    "\n",
    "\n",
    "This callback is very useful to reduce the overfitting of a model.\n",
    "\n",
    "- monitor = 'val_loss' (name of the metric to be controlled)\n",
    "\n",
    "- patience = 3 (number of epochs to wait before interrupting the training process)\n",
    "\n",
    "- mode = 'min' (allows to specify if the metric should increase or decrease: here, one should choose 'min' since the metric is a loss that we are trying to minimize)\n",
    "\n",
    "- restore_best_weights = True (to restore the weights of the best epoch in the specified metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1.000000082740371e-08.\n",
      "Epoch 1/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0146 - mean_absolute_error: 0.8545 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-08\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 1.000000082740371e-08.\n",
      "Epoch 2/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1395 - mean_absolute_error: 0.9203 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-08\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 1.000000082740371e-08.\n",
      "Epoch 3/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1184 - mean_absolute_error: 0.8994 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-08\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 1.000000082740371e-09.\n",
      "Epoch 4/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1223 - mean_absolute_error: 0.9097 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-09\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 1.000000082740371e-09.\n",
      "Epoch 5/20\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0854 - mean_absolute_error: 0.8841 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d8a71231a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=3,\n",
    "                                         mode='min',\n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32, \n",
    "          epochs=20,\n",
    "          validation_split=0.2, \n",
    "          callbacks=[early_stopping, lrScheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ModelCheckpoint callback \n",
    "\n",
    "It allows to save a model regularly during training, which can be useful during a long learning process.\n",
    "\n",
    "- filepath = filepath (path to backup folder)\n",
    "- monitor = 'val_loss' (name of the metric to be controlled)\n",
    "- save_best_only = True (so that the best model is not overwritten)\n",
    "- save_weights_only = False (so that the model does not save only the weights)\n",
    "- mode = 'min' (allows to specify if the metric must increase or decrease: here we choose 'min' because the metric is a loss to minimize)\n",
    "- save_freq = 'epoch' (so that the model is saved after each epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1101 - mean_absolute_error: 0.8901 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268\n",
      "Epoch 2/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1015 - mean_absolute_error: 0.8947 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268\n",
      "Epoch 3/5\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1342 - mean_absolute_error: 0.9154 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d8a712ed80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "filepath = cwd + \"/\"\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(filepath=filepath  + \"model.keras\", \n",
    "                                       monitor='val_loss',\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=False,\n",
    "                                       mode='min',\n",
    "                                       save_freq='epoch')\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32, \n",
    "          epochs=5, \n",
    "          validation_split=0.2, \n",
    "          callbacks=[early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceLROnPlateau callback\n",
    "\n",
    "If we want to change the learning rate according to the metric and not the epoch, we can use the ReduceLROnPlateau callback.\n",
    "\n",
    "- monitor = 'val_loss' (name of the metric to be controlled)\n",
    "- patience = 2 (number of epochs to wait before stopping the training)\n",
    "- verbose = 2 (number of information that will be displayed during the learning process)\n",
    "- mode = 'min' (allows to specify if the metric must increase or decrease: here we choose 'min' because the metric is a loss to minimize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1193 - mean_absolute_error: 0.9073 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-09\n",
      "Epoch 2/50\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0911 - mean_absolute_error: 0.8883 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-09\n",
      "Epoch 3/50\n",
      "\u001b[1m 1/20\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.4080 - mean_absolute_error: 1.0087\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1563 - mean_absolute_error: 0.9089 - val_loss: 1.1812 - val_mean_absolute_error: 0.9268 - learning_rate: 1.0000e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d8a7132ff0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_plateau = callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         patience=2,\n",
    "                                         verbose=2,\n",
    "                                         mode='min')\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size=32, \n",
    "          epochs=50,\n",
    "          validation_split=0.2, \n",
    "          callbacks=[early_stopping, lr_plateau])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the batch 0, the train error is    1.07.\n",
      "For the batch 1, the train error is    1.10.\n",
      "For the batch 2, the train error is    1.20.\n",
      "For the batch 3, the train error is    1.12.\n",
      "For the batch 4, the train error is    1.15.\n",
      "For the batch 5, the train error is    1.12.\n",
      "For the batch 6, the train error is    1.08.\n",
      "For the batch 7, the train error is    1.08.\n",
      "For the batch 8, the train error is    1.08.\n",
      "For the batch 9, the train error is    1.11.\n",
      "Validation completed.\n",
      "Loss of validation for the epoch 0 is    1.18 and the metric is    0.93.\n",
      "For the batch 0, the train error is    1.17.\n",
      "For the batch 1, the train error is    1.18.\n",
      "For the batch 2, the train error is    1.18.\n",
      "For the batch 3, the train error is    1.18.\n",
      "For the batch 4, the train error is    1.19.\n",
      "For the batch 5, the train error is    1.16.\n",
      "For the batch 6, the train error is    1.14.\n",
      "For the batch 7, the train error is    1.10.\n",
      "For the batch 8, the train error is    1.14.\n",
      "For the batch 9, the train error is    1.11.\n",
      "Validation completed.\n",
      "Loss of validation for the epoch 1 is    1.18 and the metric is    0.93.\n",
      "For the batch 0, the train error is    1.22.\n",
      "For the batch 1, the train error is    1.15.\n",
      "For the batch 2, the train error is    1.14.\n",
      "For the batch 3, the train error is    1.06.\n",
      "For the batch 4, the train error is    1.08.\n",
      "For the batch 5, the train error is    1.08.\n",
      "For the batch 6, the train error is    1.09.\n",
      "For the batch 7, the train error is    1.10.\n",
      "For the batch 8, the train error is    1.11.\n",
      "For the batch 9, the train error is    1.11.\n",
      "Validation completed.\n",
      "Loss of validation for the epoch 2 is    1.18 and the metric is    0.93.\n",
      "For the batch 0, the train error is    1.21.\n",
      "For the batch 1, the train error is    1.10.\n",
      "For the batch 2, the train error is    1.14.\n",
      "For the batch 3, the train error is    1.15.\n",
      "For the batch 4, the train error is    1.12.\n",
      "For the batch 5, the train error is    1.14.\n",
      "For the batch 6, the train error is    1.12.\n",
      "For the batch 7, the train error is    1.12.\n",
      "For the batch 8, the train error is    1.13.\n",
      "For the batch 9, the train error is    1.11.\n",
      "Validation completed.\n",
      "Loss of validation for the epoch 3 is    1.18 and the metric is    0.93.\n",
      "For the batch 0, the train error is    1.01.\n",
      "For the batch 1, the train error is    1.03.\n",
      "For the batch 2, the train error is    1.08.\n",
      "For the batch 3, the train error is    1.07.\n",
      "For the batch 4, the train error is    1.10.\n",
      "For the batch 5, the train error is    1.09.\n",
      "For the batch 6, the train error is    1.08.\n",
      "For the batch 7, the train error is    1.09.\n",
      "For the batch 8, the train error is    1.10.\n",
      "For the batch 9, the train error is    1.11.\n",
      "Validation completed.\n",
      "Loss of validation for the epoch 4 is    1.18 and the metric is    0.93.\n",
      "For the batch 0, the train error is    0.98.\n",
      "For the batch 1, the train error is    1.04.\n",
      "For the batch 2, the train error is    1.12.\n",
      "For the batch 3, the train error is    1.13.\n",
      "For the batch 4, the train error is    1.13.\n",
      "For the batch 5, the train error is    1.12.\n",
      "For the batch 6, the train error is    1.13.\n",
      "For the batch 7, the train error is    1.12.\n",
      "For the batch 8, the train error is    1.12.\n",
      "For the batch 9, the train error is    1.11.\n",
      "Validation completed.\n",
      "Loss of validation for the epoch 5 is    1.18 and the metric is    0.93.\n",
      "For the batch 0, the train error is    1.08.\n",
      "For the batch 1, the train error is    1.10.\n",
      "For the batch 2, the train error is    1.03.\n",
      "For the batch 3, the train error is    1.07.\n",
      "For the batch 4, the train error is    1.06.\n",
      "For the batch 5, the train error is    1.04.\n",
      "For the batch 6, the train error is    1.03.\n",
      "For the batch 7, the train error is    1.08.\n",
      "For the batch 8, the train error is    1.09.\n",
      "For the batch 9, the train error is    1.11.\n",
      "Validation completed.\n",
      "Loss of validation for the epoch 6 is    1.18 and the metric is    0.93.\n",
      "For the batch 0, the train error is    1.09.\n",
      "For the batch 1, the train error is    1.07.\n",
      "For the batch 2, the train error is    1.21.\n",
      "For the batch 3, the train error is    1.15.\n",
      "For the batch 4, the train error is    1.17.\n",
      "For the batch 5, the train error is    1.12.\n",
      "For the batch 6, the train error is    1.12.\n",
      "For the batch 7, the train error is    1.10.\n",
      "For the batch 8, the train error is    1.10.\n",
      "For the batch 9, the train error is    1.11.\n",
      "Validation completed.\n",
      "Loss of validation for the epoch 7 is    1.18 and the metric is    0.93.\n",
      "For the batch 0, the train error is    1.07.\n",
      "For the batch 1, the train error is    0.99.\n",
      "For the batch 2, the train error is    0.99.\n",
      "For the batch 3, the train error is    1.06.\n",
      "For the batch 4, the train error is    1.08.\n",
      "For the batch 5, the train error is    1.08.\n",
      "For the batch 6, the train error is    1.10.\n",
      "For the batch 7, the train error is    1.09.\n",
      "For the batch 8, the train error is    1.09.\n",
      "For the batch 9, the train error is    1.11.\n",
      "Validation completed.\n",
      "Loss of validation for the epoch 8 is    1.18 and the metric is    0.93.\n",
      "For the batch 0, the train error is    1.35.\n",
      "For the batch 1, the train error is    1.16.\n",
      "For the batch 2, the train error is    1.17.\n",
      "For the batch 3, the train error is    1.15.\n",
      "For the batch 4, the train error is    1.15.\n",
      "For the batch 5, the train error is    1.13.\n",
      "For the batch 6, the train error is    1.14.\n",
      "For the batch 7, the train error is    1.13.\n",
      "For the batch 8, the train error is    1.13.\n",
      "For the batch 9, the train error is    1.11.\n",
      "Validation completed.\n",
      "Loss of validation for the epoch 9 is    1.18 and the metric is    0.93.\n"
     ]
    }
   ],
   "source": [
    "class CustomCallBack(callbacks.Callback):\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"For the batch {}, the train error is {:7.2f}.\".format(batch, logs['loss']))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\"Loss of validation for the epoch {} is {:7.2f} and the metric is {:7.2f}.\".format(epoch, logs['val_loss'], logs['val_mean_absolute_error']))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        print(\"Validation completed.\")\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=64, \n",
    "                    epochs=10,\n",
    "                    validation_split=0.2, \n",
    "                    verbose=0,              #  To only take into account the messages displayed by the callback (and not by the fit method)\n",
    "                    callbacks=[CustomCallBack()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
